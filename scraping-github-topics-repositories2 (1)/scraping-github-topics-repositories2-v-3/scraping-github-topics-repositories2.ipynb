{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scraping-github-topics-repositories2\n",
    "\n",
    "Use the \"Run\" button to execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import python libraries:\n",
    "import os #intraction with files\n",
    "import requests  #to get response from url\n",
    "from bs4 import BeautifulSoup #parsing html\n",
    "import pandas as pd  #to form dataframe\n",
    "\n",
    "#this function will return a doc that has all the html parsered inside it.\n",
    "def get_topics_page():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topics_url)) \n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc \n",
    "\n",
    "# now get the title of each topic on the page and put then in a list called topic_titles.\n",
    "\n",
    "def get_topic_titles(doc):\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class': selection_class})\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles  \n",
    "\n",
    "# similarly get the topic description and put that in a list called as topic_descs.\n",
    "def get_topic_descs(doc):\n",
    "    desc_selector = 'f5 color-text-secondary mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p', {'class': desc_selector})\n",
    "    topic_descs = []\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "    return topic_descs\n",
    "\n",
    "#get the url of each topic and add them to topic_urls list.\n",
    " \n",
    "def get_topic_urls(doc):\n",
    "    topic_link_tags = doc.find_all('a', {'class': 'd-flex no-underline'})\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "    return topic_urls    \n",
    "\n",
    "#put them all together in a dictionary and return the dataframe:\n",
    "def scrape_topics():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topics_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    topics_dict = {\n",
    "        'title': get_topic_titles(doc),\n",
    "        'description': get_topic_descs(doc),\n",
    "        'url': get_topic_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict) \n",
    "\n",
    "\n",
    "docs = get_topics_page()\n",
    "titles =get_topic_titles(docs)\n",
    "desc = get_topic_descs(docs)\n",
    "url = get_topic_urls(docs)\n",
    "dataframe = scrape_topics() \n",
    "\n",
    "# Now write this infomation to csv file\n",
    "\n",
    "dataframe.to_csv('title_page_info.csv',index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n",
      "Scraping top repositories for \"3D\"\n",
      "Scraping top repositories for \"Ajax\"\n",
      "Scraping top repositories for \"Algorithm\"\n",
      "Scraping top repositories for \"Amp\"\n",
      "Scraping top repositories for \"Android\"\n",
      "Scraping top repositories for \"Angular\"\n",
      "Scraping top repositories for \"Ansible\"\n",
      "Scraping top repositories for \"API\"\n",
      "Scraping top repositories for \"Arduino\"\n",
      "Scraping top repositories for \"ASP.NET\"\n",
      "Scraping top repositories for \"Atom\"\n",
      "Scraping top repositories for \"Awesome Lists\"\n",
      "Scraping top repositories for \"Amazon Web Services\"\n",
      "Scraping top repositories for \"Azure\"\n",
      "Scraping top repositories for \"Babel\"\n",
      "Scraping top repositories for \"Bash\"\n",
      "Scraping top repositories for \"Bitcoin\"\n",
      "Scraping top repositories for \"Bootstrap\"\n",
      "Scraping top repositories for \"Bot\"\n",
      "Scraping top repositories for \"C\"\n",
      "Scraping top repositories for \"Chrome\"\n",
      "Scraping top repositories for \"Chrome extension\"\n",
      "Scraping top repositories for \"Command line interface\"\n",
      "Scraping top repositories for \"Clojure\"\n",
      "Scraping top repositories for \"Code quality\"\n",
      "Scraping top repositories for \"Code review\"\n",
      "Scraping top repositories for \"Compiler\"\n",
      "Scraping top repositories for \"Continuous integration\"\n",
      "Scraping top repositories for \"COVID-19\"\n",
      "Scraping top repositories for \"C++\"\n"
     ]
    }
   ],
   "source": [
    "'''                   get the top 25 repos from title page                   '''\n",
    "             \n",
    "\n",
    "\n",
    "def get_topic_page(topic_url): #get title page url\n",
    "    response = requests.get(topic_url)\n",
    "    content = response.text\n",
    "    with open('titlepage.html','w') as f:\n",
    "        f.write(content)\n",
    "    doc2 = BeautifulSoup(content,'html.parser')\n",
    "    return doc2\n",
    "\n",
    "def convert_star_to_int(star_str): #converting string value of  stars into int values\n",
    "    if star_str[-1] == 'k':\n",
    "        return int(float(star_str[ : -1])*1000)\n",
    "    else:\n",
    "        return int(star_str)\n",
    "\n",
    "    #function to get user name,repository nane,number of stars,repository url:\n",
    "\n",
    "def get_repo_info(repo_tags,star_tag):\n",
    "    base_url = \"https:/github.com\"\n",
    "    a_tags = repo_tags.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    stars = convert_star_to_int(star_tag.text.strip())\n",
    "    return username, repo_name,repo_url,stars \n",
    "\n",
    "#create a dictionary containing data about repositories:\n",
    "def get_topic_repos(doc2):\n",
    "    repo_tags = doc2.find_all('h1',{'class' : 'f3 color-text-secondary text-normal lh-condensed'})\n",
    "    star_tag = doc2.find_all('a',{'class':'social-count float-none'})\n",
    "    final_dic = {\n",
    "    'username':[],\n",
    "    'stars': [],\n",
    "    'repo_name':[],\n",
    "    'repo_url':[]\n",
    "    } \n",
    "\n",
    "\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i],star_tag[i])\n",
    "        final_dic['username'].append(repo_info[0])\n",
    "        final_dic['repo_name'].append(repo_info[1])\n",
    "        final_dic['repo_url'].append(repo_info[2])\n",
    "        final_dic['stars'].append(repo_info[3])\n",
    "\n",
    "    final_dic_df = pd.DataFrame(final_dic)\n",
    "    return final_dic_df\n",
    "\n",
    "#putting all the data in csv file:\n",
    "def scrape_topic(topic_url, path):\n",
    "    if os.path.exists(path):\n",
    "        print(\"The file {} already exists. Skipping...\".format(path))\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(path, index=None)\n",
    "\n",
    "\n",
    "def scrape_topics_repos():\n",
    "    print('Scraping list of topics')\n",
    "    topics_df = scrape_topics()\n",
    "    \n",
    "    for index,row in topics_df.iterrows():\n",
    "        print('Scraping top repositories for \"{}\"'.format(row['title']))\n",
    "        scrape_topic(row['url'], row['title'])\n",
    "\n",
    "scrape_topics_repos()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jovian.commit(\"scraping-github-topics-repositories2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}